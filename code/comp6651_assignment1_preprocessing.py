# -*- coding: utf-8 -*-
"""COMP6651-assignment1-preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11uQ-AzAYoWi1VAdvXZGhYzwbanqCbAza
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
import os

base_path = '/content/drive/MyDrive/COMP6651-project/'

files = {
    'D1': {'name': 'iris.csv', 'drop': ['species']},
    'D2': {'name': 'AI_index_db.csv', 'drop': ['Country', 'Region', 'Cluster', 'Income group', 'Political regime', 'Total score']},
    'D3': {'name': 'earthquakes.csv', 'drop': [
            'id', 'title', 'date', 'time', 'updated', 'url', 'detailUrl',
            'type', 'status', 'alert', 'net', 'code', 'ids', 'sources', 'types',
            'place', 'placeOnly', 'location', 'continent', 'country',
            'subnational', 'city', 'locality', 'postcode', 'what3words',
            'timezone', 'locationDetails', 'geometryType', 'magType',

            # Irrelevant
            'nst', 'gap', 'dmin', 'rms', 'distanceKM', 'felt', 'cdi', 'mmi'
        ]}
}

def preprocess_and_save(file_key):
    file_info = files[file_key]
    input_full_path = os.path.join(base_path, file_info['name'])
    output_full_path = os.path.join(base_path, f"{file_key}_Processed.csv")

    print(f"--- Processing {file_info['name']} ---")

    # 1. Load Data
    try:
        df = pd.read_csv(input_full_path)
    except FileNotFoundError:
        print(f"ERROR: Could not find file at {input_full_path}")
        print("Check if the folder name 'COMP6651-project' is correct (case sensitive!)")
        return

    # 2. Drop non-numeric/ID columns
    df_clean = df.drop(columns=file_info['drop'], errors='ignore')

    # 3. Keep only numeric columns
    df_numeric = df_clean.select_dtypes(include=[np.number])
    print(f"Original shape: {df.shape} -> Numeric shape: {df_numeric.shape}")

    if df_numeric.shape[1] == 0:
        print("Error: No numeric data left!")
        return

    # 4. Impute Missing Values (Fill NaNs with Mean)
    if df_numeric.isnull().values.any():
        print(f"Imputing missing values...")
        imputer = SimpleImputer(strategy='mean')
        data_imputed = imputer.fit_transform(df_numeric)
    else:
        data_imputed = df_numeric.values

    # 5. Normalize (StandardScaler)
    scaler = StandardScaler()
    data_scaled = scaler.fit_transform(data_imputed)

    # 6. Save back to Drive
    # We save without header/index to make it easy for algorithms to read a pure matrix
    pd.DataFrame(data_scaled).to_csv(output_full_path, index=False, header=False)
    print(f"Saved processed file to: {output_full_path}")
    print("-" * 30)

# Run for all 3 datasets
preprocess_and_save('D1')
preprocess_and_save('D2')
preprocess_and_save('D3')